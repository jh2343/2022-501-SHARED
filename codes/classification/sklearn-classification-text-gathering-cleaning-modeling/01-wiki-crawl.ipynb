{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Crawl example\n",
    "\n",
    "Author: J. Hickman\n",
    "\n",
    "- This code crawls through wikipedia to get a bunch of text data\n",
    "- The code lets the user specify search category topics.\n",
    "  - The more different the topics are, the easier the classification will be.\n",
    "  - For example, i used (pizza, metallurgy, basketball)\n",
    "- It then searches wikipedia for articles related to these topics\n",
    "- Loops over the wikipedia pages and gets the text from the wikipedia pages\n",
    "- Breaks the text into chunks (based on a user input specifying the number of sentences per chunk)\n",
    "- Each chunk is cleaned and tagged with a \"label\" (classification) and a numeric \"sentiment score\" (regression)\n",
    "- These cleaned chunks form a corpus of strings with associated tags\n",
    "\n",
    "```\n",
    "python -m pip install wikipedia_sections\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge wikipedia\n",
    "# conda install -c conda-forge wordcloud\n",
    "# python -m pip install wikipedia_sections\n",
    "\n",
    "import wikipedia\n",
    "import nltk\n",
    "import string \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE FOLLOWING IF YOU HAVEN'T DOWNLOADED THESE BEFORE\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set user parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS \n",
    "label_list=['pizza','metallurgy','basketball']\n",
    "max_num_pages=25\n",
    "sentence_per_chunk=5\n",
    "min_sentence_length=20\n",
    "\n",
    "# GET STOPWORDS\n",
    "# from nltk.corpus import stopwords\n",
    "stop_words=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# INITALIZE STEMMER+LEMITZIZER+SIA\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define text cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(text):\n",
    "\t# #FILTER OUT UNWANTED CHAR\n",
    "\tnew_text=\"\"\n",
    "\t# keep=string.printable\n",
    "\tkeep=\" abcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "\tfor character in text:\n",
    "\t\tif character.lower() in keep:\n",
    "\t\t\tnew_text+=character.lower()\n",
    "\t\telse: \n",
    "\t\t\tnew_text+=\" \"\n",
    "\ttext=new_text\n",
    "\t# print(text)\n",
    "\n",
    "\t# #FILTER OUT UNWANTED WORDS\n",
    "\tnew_text=\"\"\n",
    "\tfor word in nltk.tokenize.word_tokenize(text):\n",
    "\t\tif word not in nltk.corpus.stopwords.words('english'):\n",
    "\t\t\t#lemmatize \n",
    "\t\t\ttmp=lemmatizer.lemmatize(word)\n",
    "\t\t\t# tmp=stemmer.stem(tmp)\n",
    "\n",
    "\t\t\t# update word if there is a change\n",
    "\t\t\t# if(tmp!=word): print(tmp,word)\n",
    "\t\t\t\n",
    "\t\t\tword=tmp\n",
    "\t\t\tif len(word)>1:\n",
    "\t\t\t\tif word in [\".\",\",\",\"!\",\"?\",\":\",\";\"]:\n",
    "\t\t\t\t\t#remove the last space\n",
    "\t\t\t\t\tnew_text=new_text[0:-1]+word+\" \"\n",
    "\t\t\t\telse: #add a space\n",
    "\t\t\t\t\tnew_text+=word.lower()+\" \"\n",
    "\ttext=new_text.strip()\n",
    "\treturn text\n",
    "\n",
    "# clean_string('the word \"pizza\" first appeared in a Latin text from the town of Gaeta, then still part of the Byzantine Empire, in 997 AD; the text states that a tenant of certain property is to give the bishop of Gaeta duodecim pizze (\"twelve pizzas\") every Christmas Day, and another twelve every Easter Sunday.Suggested etymologies include:')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preform a wikipedia crawl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages for label = pizza : ['Pizza', 'Pizza Pizza', 'Pizza Hut', \"Domino's Pizza\", 'Hawaiian pizza', 'Pizza Margherita', 'Licorice Pizza', 'History of pizza', 'Mystic Pizza', 'Chicago-style pizza', 'Neapolitan pizza', '&pizza', 'Pizza (disambiguation)', 'Detroit-style pizza', 'Sicilian pizza', 'Pan pizza', 'Greenwich Pizza', 'Pizza marinara', \"Papa John's\", 'Chuck E. Cheese', 'Two Guys and a Girl', 'Roman pizza', 'Pizza delivery', 'Pizza saver', 'Pizza farm']\n",
      "\t Pizza\n",
      "\t Pizza Pizza\n",
      "\t Pizza Hut\n",
      "\t Domino's Pizza\n",
      "\t Hawaiian pizza\n",
      "\t Pizza Margherita\n",
      "\t Licorice Pizza\n",
      "\t History of pizza\n",
      "\t Mystic Pizza\n",
      "\t Chicago-style pizza\n",
      "\t Neapolitan pizza\n",
      "\t &pizza\n",
      "\t Pizza (disambiguation)\n",
      "SOMETHING WENT WRONG: Pizza (disambiguation)\n",
      "\t Detroit-style pizza\n",
      "\t Sicilian pizza\n",
      "\t Pan pizza\n",
      "\t Greenwich Pizza\n",
      "\t Pizza marinara\n",
      "\t Papa John's\n",
      "\t Chuck E. Cheese\n",
      "\t Two Guys and a Girl\n",
      "SOMETHING WENT WRONG: Two Guys and a Girl\n",
      "\t Roman pizza\n",
      "\t Pizza delivery\n",
      "\t Pizza saver\n",
      "\t Pizza farm\n",
      "Pages for label = metallurgy : ['Metallurgy', 'Tempering (metallurgy)', 'Ferrous metallurgy', 'Flux (metallurgy)', 'Bronze', 'Iron Age', 'Tin', 'Titanium', 'Nickel', 'Chemical metallurgy', 'Powder metallurgy', 'Annealing (materials science)', 'Plutonium', 'Beryllium', 'Silver', 'University of Leoben', 'Metallurgical and Materials Transactions', 'Iron metallurgy in Africa', 'List of metallurgical companies in Ukraine', 'Materials science', 'Non-ferrous metal', 'Zinc', 'Metallurgical coal', 'Scythian metallurgy', 'Extractive metallurgy']\n",
      "\t Metallurgy\n",
      "\t Tempering (metallurgy)\n",
      "\t Ferrous metallurgy\n",
      "\t Flux (metallurgy)\n",
      "\t Bronze\n",
      "\t Iron Age\n",
      "\t Tin\n",
      "SOMETHING WENT WRONG: Tin\n",
      "\t Titanium\n",
      "SOMETHING WENT WRONG: Titanium\n",
      "\t Nickel\n",
      "SOMETHING WENT WRONG: Nickel\n",
      "\t Chemical metallurgy\n",
      "\t Powder metallurgy\n",
      "\t Annealing (materials science)\n",
      "\t Plutonium\n",
      "\t Beryllium\n",
      "\t Silver\n",
      "\t University of Leoben\n",
      "\t Metallurgical and Materials Transactions\n",
      "SOMETHING WENT WRONG: Metallurgical and Materials Transactions\n",
      "\t Iron metallurgy in Africa\n",
      "\t List of metallurgical companies in Ukraine\n",
      "\t Materials science\n",
      "\t Non-ferrous metal\n",
      "\t Zinc\n",
      "\t Metallurgical coal\n",
      "\t Scythian metallurgy\n",
      "\t Extractive metallurgy\n",
      "Pages for label = basketball : ['Basketball', 'National Basketball Association', 'Small forward', 'FIBA', 'Basketball positions', 'History of basketball', 'Love & Basketball', 'List of basketball arenas', 'Basketball (ball)', 'College basketball', 'The Basketball Diaries (film)', 'Michael Jordan', 'List of basketball films', 'Basketball at the Summer Olympics', 'FIBA Basketball World Cup', \"Kuroko's Basketball\", 'Point guard', \"France women's national basketball team\", 'Basketball court', 'List of Olympic medalists in basketball', 'Jaime Fernández (basketball)', 'Naismith Memorial Basketball Hall of Fame', 'FIBA Under-19 Basketball World Cup', 'Basketball Wives', 'List of National Basketball Association single-game blocks leaders']\n",
      "\t Basketball\n",
      "\t National Basketball Association\n",
      "\t Small forward\n",
      "\t FIBA\n",
      "SOMETHING WENT WRONG: FIBA\n",
      "\t Basketball positions\n",
      "\t History of basketball\n",
      "\t Love & Basketball\n",
      "\t List of basketball arenas\n",
      "\t Basketball (ball)\n",
      "\t College basketball\n",
      "\t The Basketball Diaries (film)\n",
      "\t Michael Jordan\n",
      "\t List of basketball films\n",
      "\t Basketball at the Summer Olympics\n",
      "\t FIBA Basketball World Cup\n",
      "\t Kuroko's Basketball\n",
      "\t Point guard\n",
      "SOMETHING WENT WRONG: Point guard\n",
      "\t France women's national basketball team\n",
      "SOMETHING WENT WRONG: France women's national basketball team\n",
      "\t Basketball court\n",
      "\t List of Olympic medalists in basketball\n",
      "\t Jaime Fernández (basketball)\n",
      "\t Naismith Memorial Basketball Hall of Fame\n",
      "\t FIBA Under-19 Basketball World Cup\n",
      "\t Basketball Wives\n",
      "SOMETHING WENT WRONG: Basketball Wives\n",
      "\t List of National Basketball Association single-game blocks leaders\n"
     ]
    }
   ],
   "source": [
    "#INITIALIZE \n",
    "corpus=[]  # list of strings (input variables X)\n",
    "targets=[] # list of targets (labels or response variables Y)\n",
    "\n",
    "#--------------------------\n",
    "# LOOP OVER TOPICS \n",
    "#--------------------------\n",
    "for label in label_list:\n",
    "\n",
    "\t#SEARCH FOR RELEVANT PAGES \n",
    "\ttitles=wikipedia.search(label,results=max_num_pages)\n",
    "\tprint(\"Pages for label =\",label,\":\",titles)\n",
    "\n",
    "\t#LOOP OVER WIKI-PAGES\n",
    "\tfor title in titles:\n",
    "\t\ttry:\n",
    "\t\t\tprint(\"\t\",title)\n",
    "\t\t\twiki_page = wikipedia.page(title, auto_suggest=True)\n",
    "\n",
    "\t\t\t# LOOP OVER SECTIONS IN ARTICLE AND GET PAGE TEXT\n",
    "\t\t\tfor section in wiki_page.sections:\n",
    "\t\t\t\ttext=wiki_page.section(section); #print(text)\n",
    "\n",
    "\t\t\t\t#BREAK IN TO SENTANCES \n",
    "\t\t\t\tsentences=nltk.tokenize.sent_tokenize(text)\n",
    "\t\t\t\tcounter=0\n",
    "\t\t\t\ttext_chunk=''\n",
    "\n",
    "\t\t\t\t#LOOP OVER SENTENCES \n",
    "\t\t\t\tfor sentence in sentences:\n",
    "\t\t\t\t\tif len(sentence)>min_sentence_length:\n",
    "\t\t\t\t\t\tif(counter%sentence_per_chunk==0 and counter!=0):\n",
    "\t\t\t\t\t\t\t# PROCESS COMPLETED CHUNK \n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t# CLEAN STRING\n",
    "\t\t\t\t\t\t\ttext_chunk=clean_string(text_chunk)\n",
    "\n",
    "\t\t\t\t\t\t\t# REMOVE LABEL IF IN STRING (MAKES IT TOO EASY)\n",
    "\t\t\t\t\t\t\ttext_chunk=text_chunk.replace(label,\"\")\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t# REMOVE ANY DOUBLE SPACES\n",
    "\t\t\t\t\t\t\ttext_chunk=' '.join(text_chunk.split()).strip()\n",
    "\n",
    "\t\t\t\t\t\t\t#UPDATE CORPUS \n",
    "\t\t\t\t\t\t\tcorpus.append(text_chunk)\n",
    "\n",
    "\t\t\t\t\t\t\t#UPDATE TARGETS\n",
    "\t\t\t\t\t\t\tscore=sia.polarity_scores(text_chunk)\n",
    "\t\t\t\t\t\t\ttarget=[label,score['compound']]\n",
    "\t\t\t\t\t\t\ttargets.append(target)\n",
    "\n",
    "\t\t\t\t\t\t\t#print(\"TEXT\\n\",text_chunk,target)\n",
    "\n",
    "\t\t\t\t\t\t\t# RESET CHUNK FOR NEXT ITERATION \n",
    "\t\t\t\t\t\t\ttext_chunk=sentence\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\ttext_chunk+=sentence\n",
    "\t\t\t\t\t\t#print(\"--------\\n\", sentence)\n",
    "\t\t\t\t\t\tcounter+=1\n",
    "\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"WARNING: SOMETHING WENT WRONG:\", title);  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of text chunks =  5693\n",
      "number of targets =  5693\n",
      "                                                   text       label  sentiment\n",
      "0     word first appeared latin text town gaeta stil...       pizza     0.2732\n",
      "1     modern greek pitta bread apulia calabrian byza...       pizza     0.0000\n",
      "2     word pitta turn traced either ancient greek pi...       pizza     0.0000\n",
      "3     etymological dictionary italian language expla...       pizza     0.0000\n",
      "4                      origin latin pinsere pound stamp       pizza     0.0000\n",
      "...                                                 ...         ...        ...\n",
      "5688  katrina mcclain award first presented 2018 top...  basketball     0.6486\n",
      "5689  lisa leslie award first presented 2018 top cen...  basketball     0.9413\n",
      "5690  hall also formerly presented france pomeroy na...  basketball     0.8779\n",
      "5691  men award given since 1969 voted national asso...  basketball     0.5423\n",
      "5692                                 sporting news 2005  basketball     0.0000\n",
      "\n",
      "[5693 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#SANITY CHECKS AND PRINT TO FILE \n",
    "print(\"number of text chunks = \",len(corpus))\n",
    "print(\"number of targets = \",len(targets))\n",
    "\n",
    "tmp=[]\n",
    "for i in range(0,len(corpus)):\n",
    "    tmp.append([corpus[i],targets[i][0],targets[i][1]])\n",
    "df=pd.DataFrame(tmp)\n",
    "df=df.rename(columns={0: \"text\", 1: \"label\", 2: \"sentiment\"})\n",
    "print(df)\n",
    "df.to_csv('wiki-crawl-results.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #RELOAD FILE AND PRETEND THAT IS OUR STARTING POINT \n",
    "# df=pd.read_csv('wiki-crawl-results.csv')  \n",
    "# #print(df)\n",
    "\n",
    "# #CONVERT FROM STRING LABELS TO INTEGERS \n",
    "# labels=[]; #y1=[]; y2=[]\n",
    "# y1=[]\n",
    "# for label in df[\"label\"]:\n",
    "#     if label not in labels:\n",
    "#         labels.append(label)\n",
    "#         print(\"index =\",len(labels)-1,\": label =\",label)\n",
    "#     for i in range(0,len(labels)):\n",
    "#         if(label==labels[i]):\n",
    "#             y1.append(i)\n",
    "# y1=np.array(y1)\n",
    "\n",
    "# # CONVERT DF TO LIST OF STRINGS \n",
    "# corpus=df[\"text\"].to_list()\n",
    "# y2=df[\"sentiment\"].to_numpy()\n",
    "\n",
    "# print(\"number of text chunks = \",len(corpus))\n",
    "# print(len(y1))\n",
    "# print(corpus[0:3])\n",
    "\n",
    "# # INITIALIZE COUNT VECTORIZER\n",
    "# vectorizer=CountVectorizer()   \n",
    "\n",
    "# # RUN COUNT VECTORIZER ON OUR COURPUS \n",
    "# Xs  =  vectorizer.fit_transform(corpus)   \n",
    "# X=np.array(Xs.todense())\n",
    "\n",
    "# #CONVERT TO ONE-HOT VECTORS\n",
    "# maxs=np.max(X,axis=0)\n",
    "# X=np.ceil(X/maxs)\n",
    "\n",
    "# # DOUBLE CHECK \n",
    "# print(X.shape,y1.shape,y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ANLY501')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3bbb781ca6673b7d7a2eaec0820775eebfaab6ec1fac7365fb415515f8c23aa5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
