{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic text cleaning and analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Dr. Hickman  \n",
    "\n",
    "```\n",
    "#DEPENDENCIES\n",
    "\n",
    "#FROM COMMAND LINE\n",
    "conda install -c anaconda nltk\n",
    "\n",
    "#FROM INSIDE PYTHON RUN\n",
    "import nltk; \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download([\n",
    "    \"names\",\n",
    "    \"stopwords\",\n",
    "    \"state_union\",\n",
    "    \"twitter_samples\",\n",
    "    \"movie_reviews\",\n",
    "    \"averaged_perceptron_tagger\",\n",
    "    \"vader_lexicon\",\n",
    "    \"punkt\",\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UN-COMMENT AND RUN THE FOLLOWING CELL THE FIRST TIME YOU USE THE NOTEBOOK\n",
    "# import nltk; \n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download([\n",
    "#     \"names\",\n",
    "#     \"stopwords\",\n",
    "#     \"state_union\",\n",
    "#     \"twitter_samples\",\n",
    "#     \"movie_reviews\",\n",
    "#     \"averaged_perceptron_tagger\",\n",
    "#     \"vader_lexicon\",\n",
    "#     \"punkt\",])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORT PACKAGES \n",
    "import nltk\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "EXAMPLE: SENTENCE SEGMENTATION\n",
      "-----------------------------\n",
      "The universe is Great! I won the lottery today and I'm very happy. What is the best way to break sentences into chunks?\n",
      "['The universe is Great!', \"I won the lottery today and I'm very happy.\", 'What is the best way to break sentences into chunks?']\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------------\")\n",
    "print(\"EXAMPLE: SENTENCE SEGMENTATION\")\n",
    "print(\"-----------------------------\")\n",
    "text = \"The universe is Great! I won the lottery today and I'm very happy. What is the best way to break sentences into chunks?\"\n",
    "print(text)\n",
    "print(nltk.tokenize.sent_tokenize(text))\n",
    "# Output: ['The universe is Great!', 'I won a lottery ']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "EXAMPLE: VADAR SENTIMENT COMPUTE\n",
      "-----------------------------\n",
      "TEXT: I love NLTK, its a great library\n",
      "SCORE: {'neg': 0.0, 'neu': 0.265, 'pos': 0.735, 'compound': 0.8519}\n",
      "TEXT: I hate NLTK, its a terrible library\n",
      "SCORE: {'neg': 0.694, 'neu': 0.306, 'pos': 0.0, 'compound': -0.7783}\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------------\")\n",
    "print(\"EXAMPLE: VADAR SENTIMENT COMPUTE\")\n",
    "print(\"-----------------------------\")\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#INITIALIZE \n",
    "\n",
    "#STRING-1\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "text=\"I love NLTK, its a great library\"\n",
    "score=sia.polarity_scores(text)\n",
    "print('TEXT:',text)\n",
    "print(\"SCORE:\",score)\n",
    "\n",
    "#STRING-2\n",
    "text=\"I hate NLTK, its a terrible library\"\n",
    "score=sia.polarity_scores(text)\n",
    "print('TEXT:',text)\n",
    "print(\"SCORE:\",score)\n",
    "\n",
    "#WORDS\n",
    "print(\"---WORDS---\")\n",
    "for word in text.split():\n",
    "    print(word,sia.polarity_scores(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------\n",
      "EXAMPLE: WORD TOKENIZE\n",
      "-----------------------------\n",
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', '...', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n-----------------------------\")\n",
    "print(\"EXAMPLE: WORD TOKENIZE\")\n",
    "print(\"-----------------------------\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = '''Good muffins cost $3.88\\nin New York.  Please buy me\n",
    "... two of them.\\n\\nThanks.'''\n",
    "print(word_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------\n",
      "EXAMPLE: LEMITIZATION\n",
      "-----------------------------\n",
      "rocks --> rock\n",
      "corpora --> corpus\n",
      "better --> good\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n-----------------------------\")\n",
    "print(\"EXAMPLE: LEMITIZATION\")\n",
    "print(\"-----------------------------\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"rocks -->\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora -->\", lemmatizer.lemmatize(\"corpora\"))\n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"better -->\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------\n",
      "EXAMPLE: STEMMING\n",
      "-----------------------------\n",
      "rocks --> rock\n",
      "rocking --> rock\n",
      "rocked --> rock\n",
      "corpora --> corpora\n",
      "better --> better\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n-----------------------------\")\n",
    "print(\"EXAMPLE: STEMMING\")\n",
    "print(\"-----------------------------\")\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(\"rocks -->\", stemmer.stem(\"rocks\"))\n",
    "print(\"rocking -->\", stemmer.stem(\"rocks\"))\n",
    "print(\"rocked -->\", stemmer.stem(\"rocks\"))\n",
    "print(\"corpora -->\", stemmer.stem(\"corpora\"))\n",
    "print(\"better -->\", stemmer.stem(\"better\"))\n",
    "# a denotes adjective in \"pos\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------\n",
      "EXAMPLE: STOPWORDS\n",
      "-----------------------------\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n-----------------------------\")\n",
    "print(\"EXAMPLE: STOPWORDS\")\n",
    "print(\"-----------------------------\") \n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------\n",
      "EXAMPLE: SEARCHING TEXT\n",
      "-----------------------------\n",
      "Displaying 1 of 1 matches:\n",
      "line-1 : Starbucks has the best coffee line-2 : This is more\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n-----------------------------\")\n",
    "print(\"EXAMPLE: SEARCHING TEXT\")\n",
    "print(\"-----------------------------\")\n",
    "text = \"line-1: Starbucks has the best coffee \\n line-2: This is more text\"\n",
    "text = word_tokenize(text)\n",
    "text = nltk.Text(text)\n",
    "print(text.concordance(\"has\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------\n",
      "EXAMPLE: SYNONYMS,HYPERNYMS, AND ANTONYMS\n",
      "-----------------------------\n",
      "testing\n",
      "[Synset('testing.n.01'), Synset('testing.n.02'), Synset('examination.n.05'), Synset('test.v.01'), Synset('screen.v.01'), Synset('quiz.v.01'), Synset('test.v.04'), Synset('test.v.05'), Synset('test.v.06'), Synset('test.v.07')]\n",
      "['essay', 'examination', 'examine', 'prove', 'quiz', 'screen', 'test', 'testing', 'try', 'try_out']\n",
      "[]\n",
      "['be', 'check', 'determine', 'evaluate', 'examination', 'examine', 'experiment', 'investigation', 'score', 'take']\n",
      "SHORTEST: try\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n-----------------------------\")\n",
    "print(\"EXAMPLE: SYNONYMS,HYPERNYMS, AND ANTONYMS\")\n",
    "print(\"-----------------------------\")\n",
    "original_word='testing'\n",
    "# original_word=lemmatizer.lemmatize(original_word)\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "hypernyms = []\n",
    "print(original_word)\n",
    "syns=wordnet.synsets(original_word)\n",
    "print(syns)\n",
    "for syn in syns:\n",
    "\n",
    "\tfor h in syn.hypernyms(): \n",
    "\t\th=(h.name().split('.')[0].lower())\n",
    "\t\thypernyms.append(h)\n",
    "\tfor l in syn.lemmas():\n",
    "\t\t# print(l)\n",
    "\t\tsynonyms.append(l.name().lower())\n",
    "\t\tif l.antonyms():\n",
    "\t\t\tantonyms.append(l.antonyms()[0].name().lower())\n",
    "  \n",
    "print(sorted(set(synonyms)))\n",
    "print(sorted(set(antonyms)))\n",
    "print(sorted(set(hypernyms)))\n",
    "\n",
    "hypernyms.append(original_word)\n",
    "shortest_word=original_word\n",
    "for i in sorted(set(synonyms)):\n",
    "\tif(len(i)<len(shortest_word)): shortest_word=i \n",
    "\n",
    "print(\"SHORTEST:\",shortest_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------\n",
      "EXAMPLE: FILTERING CHARACTERS AND STOPWORDS\n",
      "-----------------------------\n",
      "STOPWORDS = \n",
      " ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "PRINTABLE = \n",
      " 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\u000b\f\n",
      "\n",
      "My dear Professor, surely a sensible person like\n",
      "yourself can call him by his name? All this You-\n",
      "Know-Who nonsense  for eleven years I have been\n",
      "trying to persuade people to call him by his proper\n",
      "name: Voldemort said Dumbledore, Professor McGonagall flinched, but\n",
      "Dumbledore, who was unsticking two lemon drops,\n",
      "seemed not to notice. It all gets so confusing if we\n",
      "keep saying You-Know-Who. I have never seen any\n",
      "reason to be frightened of saying Voldemorts name.\n",
      "\n",
      "my dear professor, surely sensible person like call name? all you- know-who nonsense eleven years i trying persuade people call proper name: voldemort said dumbledore, professor mcgonagall flinched, dumbledore, unsticking two lemon drops, seemed notice. it gets confusing keep saying you-know-who. i never seen reason frightened saying voldemorts name. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n-----------------------------\")\n",
    "print(\"EXAMPLE: FILTERING CHARACTERS AND STOPWORDS\")\n",
    "print(\"-----------------------------\")\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "import string \n",
    "import nltk \n",
    "\n",
    "# # WORDS TO REMOVE\n",
    "print(\"STOPWORDS = \\n\",nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# # CHAR TO KEEP \n",
    "print(\"\\nPRINTABLE = \\n\",string.printable)\n",
    "\n",
    "text=\"\"\"\n",
    "“My dear Professor, surely a sensible person like\n",
    "yourself can call him by his name? All this You-\n",
    "Know-Who’ nonsense — for eleven years I have been\n",
    "trying to persuade people to call him by his proper\n",
    "name: Voldemort” said Dumbledore, Professor McGonagall flinched, but\n",
    "Dumbledore, who was unsticking two lemon drops,\n",
    "seemed not to notice. “It all gets so confusing if we\n",
    "keep saying You-Know-Who.’ I have never seen any\n",
    "reason to be frightened of saying Voldemort’s name.”\n",
    "\"\"\"\n",
    "\n",
    "# #FILTER OUT UNWANTED CHAR\n",
    "new_text=\"\"\n",
    "for character in text:\n",
    "    if character in string.printable:\n",
    "        new_text+=character\n",
    "text=new_text\n",
    "print(text)\n",
    "\n",
    "# #FILTER OUT UNWANTED WORDS\n",
    "new_text=\"\"\n",
    "for word in nltk.tokenize.word_tokenize(text):\n",
    "    if word not in nltk.corpus.stopwords.words('english'):\n",
    "        if word in [\".\",\",\",\"!\",\"?\",\":\",\";\"]:\n",
    "            #remove the last space\n",
    "            new_text=new_text[0:-1]+word+\" \"\n",
    "        else: #add a space\n",
    "            new_text+=word.lower()+\" \"\n",
    "text=new_text\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------\n",
      "EXAMPLE: PARTS OF SPEECH TAGGIN (POS)\n",
      "-----------------------------\n",
      "Tokenized text:\n",
      " ['line-1', ':', 'Starbucks', 'has', 'the', 'best', 'coffee', 'line-2', ':', 'This', 'is', 'more', 'text']\n",
      "Tagged text:\n",
      " [('line-1', 'NN'), (':', ':'), ('Starbucks', 'NNP'), ('has', 'VBZ'), ('the', 'DT'), ('best', 'JJS'), ('coffee', 'NN'), ('line-2', 'NN'), (':', ':'), ('This', 'DT'), ('is', 'VBZ'), ('more', 'JJR'), ('text', 'JJ')]\n",
      "[('line-1', 'NN'), (':', ':'), ('Starbucks', 'NNP'), ('has', 'VBZ'), ('the', 'DT'), ('best', 'JJS'), ('coffee', 'NN'), ('line-2', 'NN'), (':', ':'), ('This', 'DT'), ('is', 'VBZ'), ('more', 'JJR'), ('text', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "print(\"\\n-----------------------------\")\n",
    "print(\"EXAMPLE: PARTS OF SPEECH TAGGIN (POS)\")\n",
    "print(\"-----------------------------\")\n",
    "text = \"line-1: Starbucks has the best coffee \\n line-2: This is more text\"\n",
    "text = nltk.word_tokenize(text)\n",
    "print(\"Tokenized text:\\n\", text)\n",
    "tagged = nltk.pos_tag(text)\n",
    "print(\"Tagged text:\\n\", tagged)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name entity recognition\n",
    "\n",
    "```\n",
    "conda install -c conda-forge spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc.ents (eleven years, Dumbledore, McGonagall, two, You-Know-Who, Voldemort)\n",
      "eleven years DATE\n",
      "dumbledore ORG\n",
      "mcgonagall PERSON\n",
      "two CARDINAL\n",
      "you-know-who ORG\n",
      "voldemort PERSON\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#IMPORT: SPACY FOR NAME ENTITY RECOGNITION (NER)\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "\n",
    "#INITIALIZE NER ENGINE\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "text=\"\"\"\n",
    "“My dear Professor, surely a sensible person like\n",
    "yourself can call him by his name? All this You-\n",
    "Know-Who’ nonsense — for eleven years I have been\n",
    "trying to persuade people to call him by his proper\n",
    "name: Voldemort” said Dumbledore, Professor McGonagall flinched, but\n",
    "Dumbledore, who was unsticking two lemon drops,\n",
    "seemed not to notice. “It all gets so confusing if we\n",
    "keep saying You-Know-Who.’ I have never seen any\n",
    "reason to be frightened of saying Voldemort’s name.”\n",
    "\"\"\"\n",
    "\n",
    "# RUN NER ON TEXT\n",
    "doc = nlp(text)\n",
    "print(\"doc.ents\",doc.ents)\n",
    "for X in doc.ents:\n",
    "    print(X.text.lower(),X.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some python string operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "EXAMPLE: CHANGE CASE\n",
      "-----------------------------\n",
      "THE UNIVERSE IS GREAT! I WON A LOTTERY.\n",
      "the universe is great! i won a lottery.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"-----------------------------\")\n",
    "print(\"EXAMPLE: CHANGE CASE\")\n",
    "print(\"-----------------------------\")\n",
    "text = \"The universe is Great! I won a lottery.\"\n",
    "print(text.upper())\n",
    "print(text.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------\n",
      "EXAMPLE: KEEP ONLY PRINTABLE CHAR\n",
      "-----------------------------\n",
      "some\u0000string. with\u0015 funny characters\n",
      "printable =  0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\u000b\f\n",
      "somestring. with funny characters\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n-----------------------------\")\n",
    "print(\"EXAMPLE: KEEP ONLY PRINTABLE CHAR\")\n",
    "print(\"-----------------------------\")\n",
    "s = \"some\\x00string. with\\x15 funny characters\"\n",
    "import string \n",
    "print(s)\n",
    "print(\"printable = \",string.printable)\n",
    "printable = set(string.printable); tmp=''\n",
    "for char in s:\n",
    "\tif(char in printable): tmp=tmp+char\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------\n",
      "EXAMPLE: REMOVE NEW LINES\n",
      "-----------------------------\n",
      "\n",
      " line-1: Starbucks has the best coffee \n",
      " line-2: This is more text\n",
      "['', ' line-1: Starbucks has the best coffee ', ' line-2: This is more text']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n-----------------------------\")\n",
    "print(\"EXAMPLE: REMOVE NEW LINES\")\n",
    "print(\"-----------------------------\")\n",
    "str1 = \"\\n line-1: Starbucks has the best coffee \\n line-2: This is more text\"\n",
    "print(str1)\n",
    "newstr = str1.splitlines()\n",
    "print(newstr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ANLY501')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3bbb781ca6673b7d7a2eaec0820775eebfaab6ec1fac7365fb415515f8c23aa5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
